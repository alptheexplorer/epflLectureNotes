  
\documentclass[titlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{empheq}
\usepackage{xcolor}
\usepackage{float}
\usepackage[
top    = 2.50cm,
bottom = 2.50cm,
left   = 2.75cm,
right  = 2.75cm]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{AICC 2}
\rhead{EPFL/Alp Ozen}

\newtheorem{remark}{Remark}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{prop}{[Proposition]}
\newtheorem{definition}{Definition}
\newtheorem{question}{Question}

\newcommand{\interior}[1]{%
  {\kern0pt#1}^{\mathrm{o}}%
}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\Rm}{\mathbb{R}^m}
\newcommand{\R}{\mathbb{R}}

\title{\textbf{AICC 2 - Bixio Rimoldi}}
\author{Alp Ozen}
\date{Spring 2019}
\newtheorem{example}{Example}[section]
\newtheorem{axiom}{Axiom}
\newtheorem{cor}{Corollary}

\begin{document}

\maketitle
\tableofcontents
\clearpage

\section{Week 2}

We begin by defining \textbf{entropy} as Shannon put it:

\begin{definition}\textbf{Entropy}

Note that this definition assumes base $2$ aka. binary.
$$ H(s) = - \sum_{s \in A} p(s)\log_{2}p(s) \ \text{A being our alpahabet aka. sample space}$$
and thus an equivalent definition is:
$$ H(s) = E[-\log_{2}p(s)]$$
And similarly, Shannon defines information as:
$$-\log_{2}p(s)$$
\end{definition}

For a random distribution we get:

\begin{example}
$$\forall x \in A \ p(x) = \frac{1}{|A|}, \ -\log_{2}p(s) = \log_{2}|A|$$
Hence the entropy function $H(s) = E[\log_{2}|A|]} = \underbrace{\log_{2}|A|}_{\text{do the algebra}}$
\end{example}

And now we present the \textbf{information theory inequality}

\begin{definition}\textbf{IT inequality}
$$ \log_{b}r \leq (r-1)\log_{b}(e)$$
\end{definition}

\begin{proof}
Given that $$\ln(r) \leq (r-1)$$ and that $$ ln(r) = \frac{\log_{b}(r)}{\log_{b}(e)}$$ we are done.
\end{proof}

And now we present the Entropy bound theorem:
\begin{theorem}
$$S \in A \ 0 \leq H(S) \leq \log|A|$$ 
\end{theorem}

\begin{proof}
We only show the RHS as the LHS is more or less trivial. 
Our goal is to show:

\begin{align*}
    \text{need to reach} H(s) - \log|A| \leq 0 \\
    E[-\log p(s)] - \log |A|\\
    = E[\log \frac{1}{p(s)|A|}]\\
    = \sum_{s\in A}p(s)(\log \frac{1}{p(s)|A|})\\
    \leq \underbrace{log(e)\sum[\frac{1}{|A|} - p(s)]}_{\text{using IT ineq.}} = 0
\end{align*}
\end{proof}





\end{document} 
